{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science — Tree Ensembles_ \n",
    "\n",
    "This sprint, your project is about water pumps in Tanzania. Can you predict which water pumps are faulty?\n",
    "\n",
    "# Random Forests, Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "- do feature engineering with dates\n",
    "- use scikit-learn for random forests\n",
    "- understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth\n",
    "- do ordinal encoding with high-cardinality categoricals\n",
    "- understand how categorical encodings affect trees differently compared to linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
    "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models. \n",
    "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
    "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
    "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
    "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
    "\n",
    "#### One-hot encoding isn’t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
    "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "#### category_encoders\n",
    "- Local, Anaconda: `conda install -c conda-forge category_encoders`\n",
    "- Google Colab: `pip install category_encoders`\n",
    "\n",
    "#### graphviz\n",
    "- Local, Anaconda: `conda install python-graphviz`\n",
    "- Google Colab:  `!pip install graphviz` `!apt-get install graphviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution example\n",
    "- Do feature engineering with dates\n",
    "- Clean data with outliers\n",
    "- Impute missing values\n",
    "- Use scikit-learn for decision trees\n",
    "- Get and interpret feature importances of a tree-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "LOCAL = '../data/tanzania/'\n",
    "WEB = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Tree-Ensembles/master/data/tanzania/'\n",
    "source = WEB\n",
    "\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(source + 'train_features.csv'), \n",
    "                 pd.read_csv(source + 'train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(source + 'test_features.csv')\n",
    "sample_submission = pd.read_csv(source + 'sample_submission.csv')\n",
    "\n",
    "# Split train into train & val\n",
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=42)\n",
    "\n",
    "\n",
    "def wrangle(X):\n",
    "    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    # Convert date_recorded to datetime\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    X = X.drop(columns='date_recorded')\n",
    "    \n",
    "    # Engineer feature: how many years from construction_year to date_recorded\n",
    "    X['years'] = X['year_recorded'] - X['construction_year']    \n",
    "    \n",
    "    # Drop recorded_by (never varies) and id (always varies, random)\n",
    "    X = X.drop(columns=['recorded_by', 'id'])\n",
    "    \n",
    "    # Drop duplicate columns\n",
    "    duplicate_columns = ['quantity_group']\n",
    "    X = X.drop(columns=duplicate_columns)\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these like null values\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, np.nan)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values\n",
    "    cols_with_zeros = ['construction_year', 'longitude', 'latitude', 'gps_height', 'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Get a dataframe with all train columns except the target\n",
    "train_features = train.drop(columns=[target])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector \n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_val = val[features]\n",
    "y_val = val[target]\n",
    "X_test = test[features]\n",
    "\n",
    "# Make pipeline!\n",
    "pipeline = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val, predict on test\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Write submission csv file\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('submission-03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "encoder = pipeline.named_steps['onehotencoder']\n",
    "tree = pipeline.named_steps['decisiontreeclassifier']\n",
    "feature_names = encoder.transform(X_val).columns\n",
    "importances = pd.Series(tree.feature_importances_, feature_names)\n",
    "\n",
    "# Plot feature importances\n",
    "n = 20\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='grey');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(tree, \n",
    "                           out_file=None, \n",
    "                           max_depth=2, \n",
    "                           feature_names=feature_names, \n",
    "                           class_names=tree.classes_, \n",
    "                           impurity=False, \n",
    "                           filled=True, \n",
    "                           proportion=True, \n",
    "                           rotate=True, \n",
    "                           rounded=True)\n",
    "\n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use scikit-learn for random forests\n",
    "\n",
    "[Scikit-Learn User Guide: Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do ordinal encoding with high-cardinality categoricals\n",
    "\n",
    "http://contrib.scikit-learn.org/categorical-encoding/ordinal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: [predicting golf putts](https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/)\n",
    "(1 feature, non-linear, regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "putts = pd.DataFrame(\n",
    "    columns=['distance', 'tries', 'successes'], \n",
    "    data = [[2, 1443, 1346],\n",
    "            [3, 694, 577],\n",
    "            [4, 455, 337],\n",
    "            [5, 353, 208],\n",
    "            [6, 272, 149],\n",
    "            [7, 256, 136],\n",
    "            [8, 240, 111],\n",
    "            [9, 217, 69],\n",
    "            [10, 200, 67],\n",
    "            [11, 237, 75],\n",
    "            [12, 202, 52],\n",
    "            [13, 192, 46],\n",
    "            [14, 174, 54],\n",
    "            [15, 167, 28],\n",
    "            [16, 201, 27],\n",
    "            [17, 195, 31],\n",
    "            [18, 191, 33],\n",
    "            [19, 147, 20],\n",
    "            [20, 152, 24]]\n",
    ")\n",
    "\n",
    "putts['rate of success'] = putts['successes'] / putts['tries']\n",
    "putts_X = putts[['distance']]\n",
    "putts_y = putts['rate of success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def putt_trees(max_depth=1, n_estimators=1):\n",
    "    models = [DecisionTreeRegressor(max_depth=max_depth), \n",
    "              RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)]\n",
    "    \n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        model.fit(putts_X, putts_y)\n",
    "        ax = putts.plot('distance', 'rate of success', kind='scatter', title=name)\n",
    "        ax.step(putts_X, model.predict(putts_X), where='mid')\n",
    "        plt.show()\n",
    "        \n",
    "interact(putt_trees, max_depth=(1,6,1), n_estimators=(10,40,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's \"random\" about random forests?\n",
    "1. Each tree trains on a random bootstrap sample of the data. (In scikit-learn, for `RandomForestRegressor` and `RandomForestClassifier`, the `bootstrap` parameter's default is `True`.) This type of ensembling is called Bagging.\n",
    "2. Each split considers a random subset of the features. (In scikit-learn, when the `max_features` parameter is not `None`.) \n",
    "\n",
    "For extra randomness, you can try [\"extremely randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)!\n",
    "\n",
    ">In extremely randomized trees (see [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging demo, with golf putts data\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
    "def diy_bagging(max_depth=1, n_estimators=1):\n",
    "    y_preds = []\n",
    "    for i in range(n_estimators):\n",
    "        title = f'Tree {i+1}'\n",
    "        bootstrap_sample = putts.sample(n=len(putts), replace=True).sort_values(by='distance')\n",
    "        bootstrap_X = bootstrap_sample[['distance']]\n",
    "        bootstrap_y = bootstrap_sample['rate of success']\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(bootstrap_X, bootstrap_y)\n",
    "        y_pred = tree.predict(bootstrap_X)\n",
    "        y_preds.append(y_pred)\n",
    "        ax = bootstrap_sample.plot('distance', 'rate of success', kind='scatter', title=title)\n",
    "        ax.step(bootstrap_X, y_pred, where='mid')\n",
    "        plt.show()\n",
    "        \n",
    "    ensembled = np.vstack(y_preds).mean(axis=0)\n",
    "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
    "    ax = putts.plot('distance', 'rate of success', kind='scatter', title=title)\n",
    "    ax.step(putts_X, ensembled, where='mid')\n",
    "    plt.show()\n",
    "    \n",
    "interact(diy_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go back to Tanzania Waterpumps ...\n",
    "\n",
    "#### viz2D helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz2D(fitted_model, X, feature1, feature2, num=100, title=''):\n",
    "    \"\"\"\n",
    "    Visualize model predictions as a 2D heatmap\n",
    "    For regression or binary classification models, fitted on 2 features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fitted_model : scikit-learn model, already fitted\n",
    "    X : pandas dataframe, which was used to fit model\n",
    "    feature1 : string, name of feature 1\n",
    "    feature2 : string, name of feature 2\n",
    "    target : string, name of target\n",
    "    num : int, number of grid points for each feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions: numpy array, predictions/predicted probabilities at each grid point\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html\n",
    "    https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(X[feature1].min(), X[feature1].max(), num)\n",
    "    x2 = np.linspace(X[feature2].min(), X[feature2].max(), num)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    X = np.c_[X1.flatten(), X2.flatten()]\n",
    "    if hasattr(fitted_model, 'predict_proba'):\n",
    "        predicted = fitted_model.predict_proba(X)[:,0]\n",
    "    else:\n",
    "        predicted = fitted_model.predict(X)\n",
    "    \n",
    "    plt.imshow(predicted.reshape(num, num), cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Decision Tree, Random Forest, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions\n",
    "# 1. Choose two features\n",
    "# 2. Run this code cell\n",
    "# 3. Interact with the widget sliders\n",
    "feature1 = 'longitude'\n",
    "feature2 = 'latitude'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_X_y(df, feature1, feature2, target):\n",
    "    features = [feature1, feature2]\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    X = X.fillna(X.median())\n",
    "    X = ce.OrdinalEncoder().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "def compare_models(max_depth=1, n_estimators=1):\n",
    "    models = [DecisionTreeClassifier(max_depth=max_depth), \n",
    "              RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators), \n",
    "              LogisticRegression(solver='lbfgs', multi_class='auto')]\n",
    "    \n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        model.fit(X, y)\n",
    "        viz2D(model, X, feature1, feature2, title=name)\n",
    "\n",
    "X, y = get_X_y(train, feature1, feature2, target='status_group')\n",
    "interact(compare_models, max_depth=(1,6,1), n_estimators=(10,40,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
    "\n",
    "# Instructions\n",
    "# 1. Choose two features\n",
    "# 2. Run this code cell\n",
    "# 3. Interact with the widget sliders\n",
    "\n",
    "feature1 = 'longitude'\n",
    "feature2 = 'latitude'\n",
    "\n",
    "def waterpumps_bagging(max_depth=1, n_estimators=1):\n",
    "    predicteds = []\n",
    "    for i in range(n_estimators):\n",
    "        title = f'Tree {i+1}'\n",
    "        bootstrap_sample = train.sample(n=len(train), replace=True)\n",
    "        X, y = get_X_y(bootstrap_sample, feature1, feature2, target='status_group')\n",
    "        tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        tree.fit(X, y)\n",
    "        predicted = viz2D(tree, X, feature1, feature2, title=title)\n",
    "        predicteds.append(predicted)\n",
    "    \n",
    "    ensembled = np.vstack(predicteds).mean(axis=0)\n",
    "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
    "    plt.imshow(ensembled.reshape(100, 100), cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "        \n",
    "interact(waterpumps_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand how categorical encodings affect trees differently compared to linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical exploration, 1 feature at a time\n",
    "\n",
    "Change `feature`, then re-run these cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'quantity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[feature].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=train[feature], \n",
    "            y=train['status_group']=='functional');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[feature].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [One Hot Encoding](http://contrib.scikit-learn.org/categorical-encoding/onehot.html)\n",
    "\n",
    "> Onehot (or dummy) coding for categorical features, produces one feature per category, each binary.\n",
    "\n",
    "Warning: May run slow, or run out of memory, with high cardinality categoricals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "encoded = encoder.fit_transform(X_train[[feature]])\n",
    "print(f'{len(encoded.columns)} columns')\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Ordinal Encoding](http://contrib.scikit-learn.org/categorical-encoding/ordinal.html)\n",
    "\n",
    "> Ordinal encoding uses a single column of integers to represent the classes. An optional mapping dict can be passed in; in this case, we use the knowledge that there is some true order to the classes themselves. Otherwise, the classes are assumed to have no true order and integers are selected at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder()\n",
    "encoded = encoder.fit_transform(X_train[[feature]])\n",
    "print(f'1 column, {encoded[feature].nunique()} unique values')\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "- Start a clean notebook, or continue with yesterday's assignment notebook.\n",
    "- Continue to participate in our Kaggle competition with the Tanzania Waterpumps data. \n",
    "- Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n",
    "- Try a Random Forest Classifier. \n",
    "- Try Ordinal Encoding.\n",
    "- Submit new predictions.\n",
    "- Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "- Create visualizations and share on Slack.\n",
    "- Read more about decision trees and tree ensembles. You can start with the links at the top & bottom of this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-read links\n",
    "\n",
    "#### Decision Trees\n",
    "- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
    "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
    "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
    "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
    "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n",
    "\n",
    "#### Random Forests\n",
    "- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 8: Tree-Based Methods\n",
    "- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n",
    "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)\n",
    "\n",
    "#### Categorical encoding for trees\n",
    "- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
    "- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n",
    "- [Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)\n",
    "- [Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)\n",
    "- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n",
    "- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
